{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all data needed for plots to an output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output file will be readed in DASH or Superset and must contain all the columns used to produce the plots in the static version of the [blogpost](https://unicef.sharepoint.com/:w:/r/teams/ICTD-MagicBox/DocumentLibrary1/publications/blogposts/Blogpost%20-%20government%20measures.docx?d=wb33e4efb34fb44ae94d3f07a71879e95&csf=1&web=1&e=bfTegd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GeoPandas version 0.6.2, saving a geopandas DataFrame in a CSV file works (command `geodf.to_csv()`) but reading back the data from the CSV file does not work (command `geodf = gpd.read_file()`).\n",
    "The file to save is a GeoJSON but can be changed at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "# import pickle\n",
    "import os\n",
    "# import shapefile\n",
    "from datetime import datetime, timedelta\n",
    "# import operator\n",
    "# import math\n",
    "# import copy\n",
    "# from collections import Counter, defaultdict\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.collections import LineCollection\n",
    "# from matplotlib.ticker import NullLocator, FuncFormatter\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as mcm\n",
    "import matplotlib.dates as mdates\n",
    "# from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "# from mpl_toolkits.axes_grid1.colorbar import colorbar\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# import seaborn as sns\n",
    "# from descartes import PolygonPatch\n",
    "# %matplotlib notebook\n",
    "\n",
    "## change font\n",
    "# matplotlib.rcParams['font.sans-serif'] = \"Lato\"\n",
    "# matplotlib.rcParams['font.weight'] = \"light\"\n",
    "# matplotlib.rcParams['font.size'] = 9\n",
    "# matplotlib.rcParams['axes.labelweight'] = 'bold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> CHANGE `figures_dir` and `data_dir` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "figures_dir = os.path.join(cwd, 'figures')\n",
    "# data_dir = os.path.join(cwd, 'data')\n",
    "\n",
    "data_dir = os.path.join(cwd, '../../data')\n",
    "\n",
    "# Create local `data` and `figures` folder to store the data and figures respectively\n",
    "\n",
    "# if not os.path.exists(data_dir):\n",
    "#     os.makedirs(data_dir)\n",
    "\n",
    "# if not os.path.exists(figures_dir):\n",
    "#     os.makedirs(figures_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and save data from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_from_url(url, folder_to_save, filename):\n",
    "    \n",
    "    req = requests.get(url)\n",
    "    url_content = req.content\n",
    "\n",
    "    with open(os.path.join(folder_to_save, filename), 'wb') as csv_file:\n",
    "        csv_file.write(url_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-19 data from *Our World in Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we used the dataset from ECDC but the daily reporting was discontinued (see below) so we switched to Our World in Data which sources them from John Hopkins Univesity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: From official [download page](https://www.ecdc.europa.eu/en/covid-19/data) of the dataset\n",
    "\n",
    "> **ECDC switched to a weekly reporting schedule for the COVID-19 situation worldwide and in the EU/EEA and the UK on 17 December 2020. Hence, all daily updates have been discontinued from 14 December**. ECDC will publish updates on the number of cases and deaths reported worldwide and aggregated by week every Thursday. The weekly data will be available as downloadable files in the following formats: XLSX, CSV, JSON and XML.\n",
    ">\n",
    "> With the switch from daily to weekly reporting, ECDC will shift its Epidemic Intelligence (EI) resources from case counting to signal/event detection and resume its regular EI activities, which will include COVID-19 signal and event detection and analysis but also other potential threats.  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ECDC_url = 'https://www.ecdc.europa.eu/sites/default/files/documents/COVID-19-geographic-disbtribution-worldwide.xlsx'\n",
    "\n",
    "cases_filename = 'COVID-19_ECDC.xlsx'\n",
    "\n",
    "download_data_from_url(ECDC_url, data_dir, cases_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_covid_url = 'https://covid.ourworldindata.org/data/owid-covid-data.csv'\n",
    "\n",
    "owid_filename = 'COVID-19_OWID.csv'\n",
    "\n",
    "download_data_from_url(owid_covid_url, data_dir, owid_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stringency Index by Oxford University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringency_url = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv'\n",
    "\n",
    "stringency_filename = 'OxCGRT_latest.csv'\n",
    "\n",
    "download_data_from_url(stringency_url, data_dir, stringency_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDP per capita\n",
    "\n",
    "Gross domestic product (GDP) per capita at purchasing power parity (PPP) (constant 2017 international dollars), from World Bank at https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_per_capita_url = 'https://api.worldbank.org/v2/en/indicator/NY.GDP.PCAP.PP.KD?downloadformat=excel'\n",
    "\n",
    "gdp_per_capita_filename = 'gdp_per_capita.xls'\n",
    "\n",
    "download_data_from_url(gdp_per_capita_url, data_dir, gdp_per_capita_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 data from *Our World in Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "covid_filename = 'COVID-19_OWID.csv'\n",
    "\n",
    "df_cases = pd.read_csv(os.path.join(data_dir, covid_filename), parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stringency Index for governmental policies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringency_filename = 'OxCGRT_latest.csv'\n",
    "\n",
    "df_ox = pd.read_csv(os.path.join(data_dir, stringency_filename), low_memory=False, parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDP per capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp_per_capita = pd.read_excel(os.path.join(data_dir, 'gdp_per_capita.xls'), sheet_name='Data', skiprows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map of the world countries with boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoDataframe from geopandas dataset, contains a 'geometry' column of country borders polygons + other info\n",
    "\n",
    "* Dataset `naturalearth_lowres` below is generated from a reference dataset downloaded from [here](http://www.naturalearthdata.com/downloads/110m-cultural-vectors/110m-admin-0-countries/)\n",
    "* The script that generates the dataset `naturalearth_lowres` loaded in geopandas is [here](https://github.com/geopandas/geopandas/blob/master/geopandas/datasets/naturalearth_creation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Remove Antarctica from GeoDataFrame\n",
    "gdf_world = gdf_world[gdf_world['name']!=\"Antarctica\"]\n",
    "\n",
    "# Fix missing 'iso_a3' codes for countries with 'name'\n",
    "display(gdf_world.loc[gdf_world['iso_a3'] == '-99'])\n",
    "\n",
    "gdf_world.loc[gdf_world['name'] == 'France', 'iso_a3'] = 'FRA'\n",
    "gdf_world.loc[gdf_world['name'] == 'Norway', 'iso_a3'] = 'NOR'\n",
    "gdf_world.loc[gdf_world['name'] == 'Kosovo', 'iso_a3'] = 'XXK'\n",
    "gdf_world.loc[gdf_world['name'] == 'N. Cyprus', 'iso_a3'] = 'CYP'\n",
    "gdf_world.loc[gdf_world['name'] == 'Somaliland', 'iso_a3'] = 'SOM'\n",
    "gdf_world.loc[gdf_world['name'] == 'W. Sahara', 'iso_a3'] = 'MAR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries from geopandas dataframe not in World Bank GDP per capita dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_world.loc[~gdf_world['iso_a3'].isin(df_gdp_per_capita['Country Code'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display entries from World Bank dataframe on GDP per capita which are not in geopandas gdf_world.\n",
    "\n",
    "Mostly small islands or sovereign territories, even if there are exceptions of small states, the most notable being Andorra, Bahrain, Gibraltar, San Marino, Monaco, Liechtenstein, Singapore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# df_gdp_per_capita.loc[~df_gdp_per_capita['Country Code'].isin(gdf_world['iso_a3'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official UN dataset of country boundaries\n",
    "\n",
    "* GeoJSON downloaded from the UN Geospatial Hub [here](https://geoportal.dfs.un.org/arcgis/apps/sites/#/geohub/items/c4b9b7a3bfb644d19158220ae0decb65). \n",
    "(Can be reached from the [main page](https://geoservices.un.org/webapps/geohub/) of UN Geospatial Hub, click on 'Geodata', then on the link 'UN map carto geojson' or 'UN map carto shapefile', finally on 'Download' button top right).\n",
    "\n",
    "* Alternatively, the 'UN Map' of the world provided by OCHA FISS can be downloaded from HDX after request at https://data.humdata.org/dataset/united-nations-map "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Governmental measures on policy restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dates of physical distancing or lockdown measures implemented by governments were manually selected from ACAPS Excel file listing all measures in various areas (health, physical distancing, travels ban, lockdowns, economy, ...) from https://www.acaps.org/covid-19-government-measures-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_measures():\n",
    "\n",
    "    measures = {'MYS': [(datetime(2020,3,18),'lockdown'),(datetime(2020,5,7),'measures\\nloosened')],\n",
    "                'IDN':[(datetime(2020,3,16),'physical\\ndistancing\\nenacted'), (datetime(2020,5,7),'resumption\\ndomestic\\nflights')\n",
    "                       #(datetime(2020,4,27),'ban on going\\nhome for\\nRamadan')\n",
    "                      ],\n",
    "                'IND':[#(datetime(2020,3,5),'public\\ngatherings\\nlimited'),\n",
    "                       #(datetime(2020,3,14),'emergency\\nprotocols\\ninvoked'),\n",
    "                       (datetime(2020,3,24),'lockdown'), (datetime(2020,4,25),'shops\\nreopening')],\n",
    "                'MEX':[(datetime(2020,3,14),'public\\nhealth\\nmeasures'),\n",
    "                       #(datetime(2020,3,30),'lockdown'),\n",
    "                       (datetime(2020,5,18),'partial\\nreopening')\n",
    "                       #, (datetime(2020,6,1),'total\\nreopening')\n",
    "                      ],\n",
    "                'NGA':[#(datetime(2020,3,23),'schools\\nclosed\\n(Lagos)'),\n",
    "                       (datetime(2020,3,30),'domestic\\ntravel\\nrestrictions'),\n",
    "                       (datetime(2020,6,2),'partial\\nreopening')\n",
    "                      ],\n",
    "                'COL':[(datetime(2020,3,12),'health\\nemergency\\ndeclared'),\n",
    "                       (datetime(2020,5,5),'partial\\nreopening')],\n",
    "                'CIV':[(datetime(2020,3,24),'national\\ncurfew'),\n",
    "                       (datetime(2020,5,8),'total\\nreopening')],\n",
    "                'MMR':[(datetime(2020,3,13),'suspension of\\npublic gatherings'),(pd.NaT,'')],\n",
    "                'MOZ':[(datetime(2020,3,23),'limit public\\ngatherings'),\n",
    "                       (datetime(2020,6,28),'international\\nflights\\nallowed')\n",
    "                      ],\n",
    "                'UKR':[(datetime(2020,3,15),'lockdown'),\n",
    "                       (datetime(2020,5,22),'partial\\nreopening')],\n",
    "                'AUS':[(datetime(2020,3,29),'lockdown'),\n",
    "                       (datetime(2020,5,22),'partial\\nreopening')],\n",
    "                'DEU':[(datetime(2020,3,16),'non-essential\\npublic\\nservices\\nclosed'),\n",
    "                       (datetime(2020,5,15),'internal\\ntransport\\nresumed')],\n",
    "                'GBR':[(datetime(2020,3,24),'lockdown'),\n",
    "                       (datetime(2020,5,10),'go to\\nwork\\nallowed')],\n",
    "                'ITA':[(datetime(2020,3,23),'lockdown'),\n",
    "                       (datetime(2020,5,10),'businesses\\nreopening')],\n",
    "                'SGP':[(datetime(2020,4,7),'limit public gatherings'),\n",
    "                       (datetime(2020,6,19),'businesses reopening')]\n",
    "               }\n",
    "    \n",
    "    \n",
    "    abbr_to_name = {'CIV':\"CÃ´te d'Ivoire\",\n",
    "                'COL':'Colombia',\n",
    "                'IDN':'Indonesia',\n",
    "                'IND':'India',\n",
    "                'MEX':'Mexico',\n",
    "                'MMR':'Myanmar',\n",
    "                'MOZ':'Mozambique',\n",
    "                'MYS':'Malaysia',\n",
    "                'NGA':'Nigeria',\n",
    "                'UKR':'Ukraine',\n",
    "                'AUS':'Australia',\n",
    "                'DEU':'Germany',\n",
    "                'GBR':'United Kingdom',\n",
    "                'ITA':'Italy',\n",
    "                'SGP':'Singapore',\n",
    "                'ESP':'Spain',\n",
    "                'SWE':'Sweden',\n",
    "                'KOR':'South Korea',\n",
    "                'IRL':'Ireland'\n",
    "               }\n",
    "\n",
    "    \n",
    "    country_code = []\n",
    "    start_date = []\n",
    "    end_date = []\n",
    "    start_string = []\n",
    "    end_string = []\n",
    "    country_name = []\n",
    "    \n",
    "    for cc,m in measures.items():\n",
    "        country_code.append(cc)\n",
    "        start_date.append(m[0][0])\n",
    "        start_string.append(m[0][1])\n",
    "        end_date.append(m[1][0])\n",
    "        end_string.append(m[1][1])\n",
    "        country_name.append(abbr_to_name[cc])\n",
    "    \n",
    "    df_measures = pd.DataFrame({'iso_a3':country_code,'country':country_name,\n",
    "                                'start_date':start_date, 'start_string':start_string, \n",
    "                                'end_date':end_date, 'end_string':end_string},\n",
    "                               columns = ['iso_a3', 'country', 'start_date', 'start_string', 'end_date', 'end_string'])\n",
    "    \n",
    "    df_measures.to_csv(os.path.join(data_dir, 'dates_government_measures.csv'), index=False)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_measures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to save/write datasets for different plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge into a single dataframe the COVID-19 dataset and the Stringency Index dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess COVID-19 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keep only non-negative `cases` and `deaths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = df_cases.loc[df_cases['new_cases'] >= 0]\n",
    "df_cases = df_cases.loc[df_cases['new_deaths'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate `cases` and `deaths` per 100k population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases['new_cases_per_100000'] = (df_cases['new_cases'].astype('float')*100000)/df_cases['population']\n",
    "df_cases['new_deaths_per_100000'] = (df_cases['new_deaths'].astype('float')*100000)/df_cases['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate 7 day averages for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(s_input, window):\n",
    "    return s_input.rolling(window=window, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_country_df_cases = df_cases.groupby('iso_code')\n",
    "                                                            \n",
    "sel_cols = ['new_cases', 'new_deaths', 'new_cases_per_100000', 'new_deaths_per_100000']\n",
    "\n",
    "for col in sel_cols:\n",
    "    df_cases[col+'_7_day_average'] = grouped_country_df_cases[col].apply(lambda x: moving_average(x,7))\n",
    "    df_cases[col+'_7_day_average_daily_change'] = df_cases[col+'_7_day_average'].pct_change()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check moving average applied to grouped_country_df_cases is what expected for a single contry\n",
    "\n",
    "# df_cases_country = df_cases.loc[df_cases['iso_code'] == 'BRA']\n",
    "# s_moving_avg = df_cases_country['new_cases'].rolling(window=7, min_periods=1).mean()\n",
    "# display(s_moving_avg.equals(df_cases_country['new_cases_7_day_average']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Stringency Index dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The stringency index may have been produced at subnational level for few countries (Brazil, Canada, USA, UK), so subnational indexes must be excluded since would result in multiple values (national and subnationals) for the same country. We only want to keep the national stringency index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Countries with subnational stringency index\n",
    "# df_ox.loc[df_ox['RegionName'].notnull()][['CountryName']].drop_duplicates()\n",
    "\n",
    "df_ox = df_ox.drop(df_ox.loc[df_ox['RegionName'].notnull()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the Stringency Index into the COVID-19 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_df_ox_keep = ['CountryCode','Date','StringencyIndexForDisplay']\n",
    "\n",
    "columns_df_ox_merge = ['CountryCode', 'Date']\n",
    "columns_df_cases_merge = ['iso_code','date']\n",
    "\n",
    "df_merged_cases_stringency = pd.merge(df_cases, df_ox[columns_df_ox_keep], left_on=columns_df_cases_merge, right_on=columns_df_ox_merge, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save later the dataset since we will insert a flag column to reproduce the heatmap in the blogpost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of countries vs first day of maximum stringency index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_hist_countries_vs_date_max_stringency(df_input, freq_datespan, colormap):\n",
    "    \n",
    "    only_national = df_input['RegionName'].isnull()\n",
    "    \n",
    "    df_national = df_input.loc[only_national]\n",
    "    grp_national = df_national.groupby('CountryName')\n",
    "    \n",
    "    colmax = grp_national['StringencyIndexForDisplay'].max() \n",
    "    \n",
    "    list_first_date_max_stringency = []\n",
    "    \n",
    "    # df_date_edges = pd.date_range(start = df_input['Date'].min(), end = df_input['Date'].max(), freq = '7D')\n",
    "    # list_date_edges = np.arange(df_input['Date'].min(), df_input['Date'].max(), dtype='datetime64[W]')\n",
    "\n",
    "    min_date, max_date = df_input['Date'].min(), df_input['Date'].max()\n",
    "    \n",
    "    for name, group in grp_national:\n",
    "        indx_max_stringency = group['StringencyIndexForDisplay'] == group['StringencyIndexForDisplay'].max()\n",
    "        # Get the first day of max strinngency\n",
    "        indx_first_date_max_stringency = group.loc[indx_max_stringency]['Date'].min()\n",
    "        list_first_date_max_stringency.append(indx_first_date_max_stringency.to_datetime64())\n",
    "    \n",
    "    df_first_date_max_stringency = pd.Series(sorted(list_first_date_max_stringency), dtype='datetime64[ns]')\n",
    "    \n",
    "    # Check if the year is always the same, since df_first_date_max_stringency.dt.week is the week number of that year.\n",
    "    # If there are different years, then a reference to this info should be included in the plot from the weekday\n",
    "    if df_first_date_max_stringency.dt.year.nunique() == 1:\n",
    "\n",
    "        bottom_date = '2020-03-01'\n",
    "        top_date = '2020-08-30'\n",
    "        \n",
    "        ## pd.date_range(min_date, max_date, freq='W-MON') below starts from Sunday (excluded), could start from any other day (excluded) using freq='W-MON' for example. \n",
    "        ## If min_date it is after then the first week is incomplete and does not get inside the range\n",
    "        # df_dates_range = pd.date_range(min_date, max_date, freq='W-MON')        \n",
    "        res = df_first_date_max_stringency.groupby(pd.cut(df_first_date_max_stringency, \n",
    "                                                              pd.date_range(min_date, max_date, freq=freq_datespan))).count()\n",
    "\n",
    "        aggr_date_range = pd.date_range(min_date, bottom_date, periods=1).append(pd.date_range(bottom_date, top_date, freq=freq_datespan)).append(pd.DatetimeIndex([max_date]))\n",
    "        list_aggr_date_range = aggr_date_range.tolist()\n",
    "        \n",
    "        res_aggr = df_first_date_max_stringency.groupby(pd.cut(df_first_date_max_stringency, aggr_date_range)).count()\n",
    "\n",
    "        # Splitting categorical index of res_aggr into columns\n",
    "        df_to_save = res_aggr.index.categories.astype(str).to_series().str.split(', ', expand=True)\n",
    "        df_to_save = df_to_save.rename(columns={0:'start_date',1:'end_date'}).replace(\"\\]|\\(\", \"\", regex=True)\n",
    "        \n",
    "        # Adding counts for countries in interval dates from res_aggr\n",
    "        df_to_save['num_countries'] = res_aggr.values\n",
    "        \n",
    "        ## Define discrete colors for the colorbar and for bars of the histogram plot.\n",
    "        # Discard first and last dates since I don't want to plot them since they are at the limits of the range\n",
    "        bounds = [mdates.date2num(i + timedelta(days=1)) for i in list_aggr_date_range[1:-1:2]]\n",
    "        # mdates.date2num(datetime.strptime(i, \"%Y-%m-%d\"))\n",
    "        out_bounds_min, out_bounds_max = mdates.date2num(list_aggr_date_range[0]), mdates.date2num(list_aggr_date_range[-1])\n",
    "        \n",
    "        cmap = mcm.get_cmap(colormap)  # define the colormap\n",
    "        \n",
    "        # extract all colors from the .jet map\n",
    "        cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "        # create the new map\n",
    "        custom_cmap = mcolors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "        # define the bins and normalize\n",
    "        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "            \n",
    "        # Define colors for the bars from the custom colormap. Bars have the same color 2 by 2 since once bar is one week while the colormap has one color for 2 weeks.\n",
    "        custom_colors = [custom_cmap(norm(mdates.date2num(c.left + timedelta(days=1)))) for c in res_aggr.index.categories]            \n",
    "        # custom_colors[0] = mcolors.to_rgba('0.9')\n",
    "        custom_colors[0] = mcolors.to_rgba('0.7')\n",
    "        custom_colors[-1] = mcolors.to_rgba('0.7')\n",
    "        \n",
    "        df_to_save['color'] = [mcolors.to_hex(c) for c in custom_colors]\n",
    "        \n",
    "        save_path = os.path.join(data_dir, 'histo_num_countries_vs_first_day_max_stringency.csv')\n",
    "\n",
    "        df_to_save.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_hist_countries_vs_date_max_stringency(df_ox.loc[df_ox['Date'] < '2020-10-16'], 'W', 'inferno_r') #'viridis', 'cividis', 'plasma'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographical maps of countries colored according to a given value/indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_geodataframe_first_day_max_stringency_cases_per_100k(gdf_input, df_stringency, df_cases, col_df_cases, freq_datespan, colormap_first_day_max_stringency):    \n",
    "    \n",
    "    only_national = df_stringency['RegionName'].isnull()\n",
    "    \n",
    "    df_stringency_national = df_stringency.loc[only_national]\n",
    "    grp_stringency_national = df_stringency_national.groupby('CountryCode')\n",
    "        \n",
    "    # df_date_edges = pd.date_range(start = df_input['Date'].min(), end = df_input['Date'].max(), freq = '7D')\n",
    "    # list_date_edges = np.arange(df_input['Date'].min(), df_input['Date'].max(), dtype='datetime64[W]')\n",
    "    \n",
    "    min_date, max_date = df_stringency['Date'].min(), df_stringency['Date'].max()\n",
    "    \n",
    "    # Create a DataFrame with country as index and first day of max stringency as values in the column\n",
    "    \n",
    "    list_first_day_max_stringency = []\n",
    "    list_country_code = []\n",
    "    list_total_cases_per_100k_at_first_day_max_stringency = []\n",
    "    list_max_stringency = []\n",
    "    list_first_day_max_moving_avg_cases = []\n",
    "    # Catch the date of stringency values close to the maximum (within max stringency - 10)\n",
    "    list_first_day_sim_max_stringency = []\n",
    "\n",
    "    \n",
    "    for name, group in grp_stringency_national:\n",
    "        indx_max_stringency = group['StringencyIndexForDisplay'] == group['StringencyIndexForDisplay'].max()\n",
    "        list_max_stringency.append(group['StringencyIndexForDisplay'].max())\n",
    "        # Get the first day of max strinngency\n",
    "        list_country_code.append(name)\n",
    "        indx_first_day_max_stringency = group.loc[indx_max_stringency]['Date'].min()\n",
    "        list_first_day_max_stringency.append(indx_first_day_max_stringency.to_datetime64())\n",
    "        \n",
    "        df_cases_name = df_cases.loc[df_cases['iso_code'] == name]\n",
    "        indx_dates_before_date_max_stringency = df_cases_name['date'] <= indx_first_day_max_stringency\n",
    "        total_cases_per_100k = df_cases_name.loc[indx_dates_before_date_max_stringency][col_df_cases].sum()\n",
    "        list_total_cases_per_100k_at_first_day_max_stringency.append(total_cases_per_100k)\n",
    "\n",
    "    df_first_day_max_stringency = pd.DataFrame({'first_day_max_stringency':list_first_day_max_stringency,\n",
    "                                                 'first_day_max_stringency_numerical': [mdates.date2num(i) for i in list_first_day_max_stringency],\n",
    "                                                 'total_cases_per_100k_at_first_day_max_stringency' : list_total_cases_per_100k_at_first_day_max_stringency,\n",
    "                                                 'max_stringency' : list_max_stringency\n",
    "                                                },\n",
    "                                                index=list_country_code)\n",
    "\n",
    "    ## Easy way to put one more column in the GeoDataFrame then used to plot borders and colors/value from the new columns\n",
    "    ## Merging df_first_day_max_stringency on gdf_input\n",
    "    gdf_merged = gdf_input.merge(df_first_day_max_stringency, left_on = 'iso_a3', right_on = df_first_day_max_stringency.index, how='left')\n",
    "    \n",
    "    # Check if the year is always the same, since df_first_day_max_stringency.dt.week is the week number of that year.\n",
    "    # If there are different years, then a reference to this info should be included in the plot from the weekday\n",
    "    if gdf_merged['first_day_max_stringency'].dt.year.nunique() == 1:\n",
    "\n",
    "        bottom_date = '2020-03-01'\n",
    "        top_date = '2020-08-30'\n",
    "        \n",
    "        ## pd.date_range(min_date, max_date, freq='W-MON') below starts from Sunday (excluded), could start from any other day (excluded) using freq='W-MON' for example. \n",
    "        ## If min_date it is after then the first week is incomplete and does not get inside the range\n",
    "        # df_dates_range = pd.date_range(min_date, max_date, freq='W-MON')        \n",
    "        # res = gdf_merged['first_day_max_stringency'].groupby(pd.cut(gdf_merged['first_day_max_stringency'], \n",
    "        #                                                       pd.date_range(min_date, max_date, freq=freq_datespan))).count()\n",
    "\n",
    "        \n",
    "        ## All dates including single group per all dates before bottom_date and another after top_date\n",
    "        aggr_date_range = pd.date_range(min_date, bottom_date, periods=1).append(pd.date_range(bottom_date, top_date, freq=freq_datespan)).append(pd.DatetimeIndex([max_date]))\n",
    "        list_aggr_date_range = aggr_date_range.tolist()\n",
    "        res_aggr = gdf_merged['first_day_max_stringency'].groupby(pd.cut(gdf_merged['first_day_max_stringency'], aggr_date_range)).count()\n",
    "        ## Define discrete colors for the colorbar and for bars of the histogram plot.\n",
    "        ## Discard first and last dates since I don't want to plot them since they are at the limits of the range\n",
    "        bounds_first_day_max_stringency = [mdates.date2num(i + timedelta(days=1)) for i in list_aggr_date_range[1:-1:2]]\n",
    "        out_bounds_first_day_max_stringency_min, out_bounds_first_day_max_stringency_max = mdates.date2num(list_aggr_date_range[0]), mdates.date2num(list_aggr_date_range[-1])            \n",
    "        \n",
    "        # Instead of the interval in days corresponding to the week, get only the corresponding Monday to then substitute the x ticks\n",
    "        # monday_of_week = [(c.left + timedelta(days=1)) for c in res.index.categories]\n",
    "        # monday_of_week_str = [date.strftime('%d.%m') for date in monday_of_week]\n",
    "\n",
    "        ## Group dates of max stringency by week number of the year\n",
    "        # count_first_day_max_stringency_per_week = df_first_day_max_stringency.groupby(df_first_day_max_stringency.dt.week).count()\n",
    "        # count_first_day_max_stringency_per_week.plot(kind='bar', ax=ax)\n",
    "        # display(df_first_day_max_stringency.apply(lambda x: x - timedelta(days=x.weekday())))\n",
    "\n",
    "        cmap_first_day_max_stringency = mcm.get_cmap(colormap_first_day_max_stringency)  # define the colormap\n",
    "        # extract all colors from the .jet map\n",
    "        cmaplist_first_day_max_stringency = [cmap_first_day_max_stringency(i) for i in range(cmap_first_day_max_stringency.N)]\n",
    "\n",
    "        # create the new map\n",
    "        custom_cmap_first_day_max_stringency = mcolors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist_first_day_max_stringency, cmap_first_day_max_stringency.N)\n",
    "\n",
    "        # define the bins and normalize\n",
    "        norm_first_day_max_stringency = mcolors.BoundaryNorm(bounds_first_day_max_stringency, cmap_first_day_max_stringency.N)\n",
    "\n",
    "        ## Define colors for the bars from the custom colormap. Bars have the same color 2 by 2 since once bar is one week while the colormap has one color for 2 weeks.\n",
    "        custom_colors_first_day_max_stringency = [custom_cmap_first_day_max_stringency(norm_first_day_max_stringency(mdates.date2num(c.left + timedelta(days=1)))) for c in res_aggr.index.categories]            \n",
    "        \n",
    "        custom_colors_first_day_max_stringency[0] = mcolors.to_rgba('0.7')\n",
    "        custom_colors_first_day_max_stringency[-1] = mcolors.to_rgba('0.7')\n",
    "        \n",
    "        gdf_merged['color_first_day_max_stringency'] = gdf_merged['first_day_max_stringency_numerical'].apply(lambda x: mcolors.to_hex(custom_cmap_first_day_max_stringency(norm_first_day_max_stringency(x))))\n",
    "        \n",
    "        ## Plot total cases per 100k population at first day of max stringency\n",
    "        indx_nnz = gdf_merged['total_cases_per_100k_at_first_day_max_stringency'] > 0\n",
    "        min_total_cases_per_100k = gdf_merged.loc[indx_nnz]['total_cases_per_100k_at_first_day_max_stringency'].min()\n",
    "        max_total_cases_per_100k = gdf_merged.loc[indx_nnz]['total_cases_per_100k_at_first_day_max_stringency'].max()\n",
    "        \n",
    "        indx_null = gdf_merged['first_day_max_stringency_numerical'].isnull()\n",
    "        indx_ls_min_bounds = gdf_merged['first_day_max_stringency_numerical'] < min(bounds_first_day_max_stringency)\n",
    "        indx_gt_max_bounds = gdf_merged['first_day_max_stringency_numerical'] > max(bounds_first_day_max_stringency)\n",
    "\n",
    "        ## Substituting dates outside of the range with null values\n",
    "        gdf_merged.loc[indx_ls_min_bounds,'first_day_max_stringency_numerical'] = np.nan\n",
    "        gdf_merged.loc[indx_gt_max_bounds,'first_day_max_stringency_numerical'] = np.nan\n",
    "        \n",
    "        gdf_merged.loc[indx_ls_min_bounds,'first_day_max_stringency'] = pd.NaT\n",
    "        gdf_merged.loc[indx_gt_max_bounds,'first_day_max_stringency'] = pd.NaT\n",
    "\n",
    "        gdf_merged.loc[indx_ls_min_bounds,'total_cases_per_100k_at_first_day_max_stringency'] = np.nan\n",
    "        gdf_merged.loc[indx_gt_max_bounds,'total_cases_per_100k_at_first_day_max_stringency'] = np.nan\n",
    "    \n",
    "        # save_path = os.path.join(data_dir, \"geodata_cases_max_stringency.geojson\")\n",
    "        # gdf_merged.to_file(save_path, driver='GeoJSON')\n",
    "        \n",
    "    return gdf_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_first_day_max_stringency_cases_per_100k = save_geodataframe_first_day_max_stringency_cases_per_100k(gdf_world, df_ox.loc[df_ox['Date'] < '2020-10-16'], df_cases, 'new_cases_per_100000', 'W', 'inferno_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_geodataframe_delay_max_cases_per_100k_and_first_day_max_stringency(gdf_input, df_stringency, df_cases, col_df_cases, bottom_date, top_date, bin_width_days, colormap):\n",
    "        \n",
    "    only_national = df_stringency['RegionName'].isnull()\n",
    "    \n",
    "    df_stringency_national = df_stringency.loc[only_national]\n",
    "    df_stringency_national = df_stringency_national.loc[(df_stringency_national['Date'] >= bottom_date) & (df_stringency_national['Date'] <= top_date)]\n",
    "    grp_stringency_national = df_stringency_national.groupby('CountryCode')\n",
    "    \n",
    "    df_cases = df_cases.loc[(df_cases['date'] >= bottom_date) & (df_cases['date'] <= top_date)] \n",
    "    \n",
    "    # df_date_edges = pd.date_range(start = df_input['Date'].min(), end = df_input['Date'].max(), freq = '7D')\n",
    "    # list_date_edges = np.arange(df_input['Date'].min(), df_input['Date'].max(), dtype='datetime64[W]')\n",
    "    \n",
    "    # min_date, max_date = df_stringency['Date'].min(), df_stringency['Date'].max()\n",
    "    \n",
    "    # Create a DataFrame with country as index and first day of max stringency as values in the column\n",
    "    \n",
    "    list_first_day_max_stringency = []\n",
    "    list_country_code = []\n",
    "    list_first_day_max_moving_avg_cases = []\n",
    "    list_max_stringency = []\n",
    "    # Catch the date of stringency values close to the maximum (within max stringency - 10)\n",
    "    list_first_day_sim_max_stringency = []\n",
    "    \n",
    "    for name, group in grp_stringency_national:\n",
    "        max_stringency = group['StringencyIndexForDisplay'].max()\n",
    "        indx_max_stringency = group['StringencyIndexForDisplay'] == max_stringency\n",
    "        list_max_stringency.append(max_stringency)\n",
    "        list_country_code.append(name)\n",
    "        ## Get the first day of max strinngency\n",
    "        indx_first_day_max_stringency = group.loc[indx_max_stringency]['Date'].min()\n",
    "        list_first_day_max_stringency.append(indx_first_day_max_stringency.to_datetime64())\n",
    "        ## Get first day with stringency similar to the maximum one\n",
    "        ## Before filtering on max_stringency >= 50 but Japan (which did a good job) is < 50, so I removed the if condition\n",
    "        # if max_stringency >= 50:\n",
    "        indx_sim_max_stringency = group['StringencyIndexForDisplay'] > max_stringency-15\n",
    "        indx_first_day_sim_max_stringency = group.loc[indx_sim_max_stringency]['Date'].min()\n",
    "        list_first_day_sim_max_stringency.append(indx_first_day_sim_max_stringency.to_datetime64())\n",
    "\n",
    "        #\n",
    "        df_cases_name = df_cases.loc[df_cases['iso_code'] == name]\n",
    "        if len(df_cases_name) == 0:\n",
    "            day_max_moving_avg_cases = pd.NaT\n",
    "        else:\n",
    "            df_cases_name_moving_avg = df_cases_name[col_df_cases].rolling(window=7, min_periods=1).mean()\n",
    "            indx_day_max_moving_avg_cases = df_cases_name_moving_avg.idxmax()\n",
    "            day_max_moving_avg_cases = df_cases_name.loc[indx_day_max_moving_avg_cases]['date']\n",
    "        \n",
    "        list_first_day_max_moving_avg_cases.append(day_max_moving_avg_cases.to_datetime64())\n",
    "\n",
    "    df_first_day_max_stringency = pd.DataFrame({'first_day_max_stringency':list_first_day_max_stringency,\n",
    "                                                 'first_day_max_stringency_numerical': [mdates.date2num(i) for i in list_first_day_max_stringency],\n",
    "                                                 'first_day_max_' + col_df_cases : list_first_day_max_moving_avg_cases,\n",
    "                                                 'max_stringency' : list_max_stringency,\n",
    "                                                 'first_day_sim_max_stringency':list_first_day_sim_max_stringency\n",
    "                                                },\n",
    "                                                index=list_country_code)\n",
    "    \n",
    "    ## Adding column for the difference in days between max cases and max stringency\n",
    "    df_first_day_max_stringency['delay_day_max_' + col_df_cases + '_first_day_max_stringency'] = df_first_day_max_stringency['first_day_max_' + col_df_cases] - df_first_day_max_stringency['first_day_max_stringency']\n",
    "    df_first_day_max_stringency['delay_day_max_' + col_df_cases + '_first_day_max_stringency_numerical'] = df_first_day_max_stringency['delay_day_max_' + col_df_cases + '_first_day_max_stringency'].dt.days\n",
    "    # display(df_first_day_max_stringency.loc[df_first_day_max_stringency['delay_day_max_' + col_df_cases + '_first_day_max_stringency_numerical'] < 0])\n",
    "    \n",
    "    df_first_day_max_stringency['delay_day_max_' + col_df_cases + '_first_day_sim_max_stringency'] = df_first_day_max_stringency['first_day_max_' + col_df_cases] - df_first_day_max_stringency['first_day_sim_max_stringency']\n",
    "    df_first_day_max_stringency['delay_day_max_' + col_df_cases + '_first_day_sim_max_stringency_numerical'] = df_first_day_max_stringency['delay_day_max_' + col_df_cases + '_first_day_sim_max_stringency'].dt.days\n",
    "\n",
    "    ## DELETE the reduntant columns \n",
    "    # 'delay_day_max_' + col_df_cases + '_first_day_max_stringency', 'delay_day_max_' + col_df_cases + '_first_day_sim_max_stringency'\n",
    "    ## since they have been converted in the numerical counterpart of number of days and are not used later,\n",
    "    ## plus they contain the type pandas._libs.tslibs.timedeltas.Timedelta which cannot be converted/saved into a GeoJSON\n",
    "    \n",
    "    columns_to_drop = ['delay_day_max_' + col_df_cases + '_first_day_max_stringency', 'delay_day_max_' + col_df_cases + '_first_day_sim_max_stringency']\n",
    "    df_first_day_max_stringency = df_first_day_max_stringency.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # display(df_first_day_max_stringency.loc[df_first_day_max_stringency['delay_day_max_' + col_df_cases + '_first_day_sim_max_stringency_numerical'] < 0])\n",
    "    \n",
    "    col_to_plot = 'delay_day_max_' + col_df_cases + '_first_day_sim_max_stringency_numerical'\n",
    "    col_ref = 'first_day_sim_max_stringency'\n",
    "    \n",
    "    ## Other valid pair is\n",
    "    # col_to_plot = 'delay_day_max_' + col_df_cases + '_first_day_max_stringency_numerical'\n",
    "    # col_ref = 'first_day_max_stringency'\n",
    "    \n",
    "    ## Plot histogram of delay in days\n",
    "    # fig_hist, ax_hist = plt.subplots(1,1,figsize=(6,4))\n",
    "    # df_first_day_max_stringency[col_to_plot].hist(bins=np.arange(df_first_day_max_stringency[col_to_plot].min(), df_first_day_max_stringency[col_to_plot].max()+1, 7), ax=ax_hist)\n",
    "    \n",
    "    ## Easy way to put one more column in the GeoDataFrame then used to plot borders and colors/value from the new columns\n",
    "    ## Merging df_first_day_max_stringency on gdf_input\n",
    "    gdf_merged = gdf_input.merge(df_first_day_max_stringency, left_on = 'iso_a3', right_on = df_first_day_max_stringency.index, how='left')\n",
    "    \n",
    "    # Check if the year is always the same, since df_first_day_max_stringency.dt.week is the week number of that year.\n",
    "    # If there are different years, then a reference to this info should be included in the plot from the weekday\n",
    "    if gdf_merged[col_ref].dt.year.nunique() == 1:\n",
    "        \n",
    "        ## pd.date_range(min_date, max_date, freq='W-MON') below starts from Sunday (excluded), could start from any other day (excluded) using freq='W-MON' for example. \n",
    "        ## If min_date it is after then the first week is incomplete and does not get inside the range\n",
    "        # df_dates_range = pd.date_range(min_date, max_date, freq='W-MON')        \n",
    "        # res = gdf_merged['first_day_max_stringency'].groupby(pd.cut(gdf_merged['first_day_max_stringency'], \n",
    "        #                                                       pd.date_range(min_date, max_date, freq=freq_datespan))).count()\n",
    "\n",
    "        \n",
    "        ## All dates including single group per all dates before bottom_date and another after top_date\n",
    "        # aggr_date_range = pd.date_range(min_date, bottom_date, periods=1).append(pd.date_range(bottom_date, top_date, freq=freq_datespan)).append(pd.DatetimeIndex([max_date]))\n",
    "        # list_aggr_date_range = aggr_date_range.tolist()\n",
    "        # res_aggr = gdf_merged['first_day_max_stringency'].groupby(pd.cut(gdf_merged['first_day_max_stringency'], aggr_date_range)).count()        \n",
    "        ## Define discrete colors for the colorbar and for bars of the histogram plot.\n",
    "        # Discard first and last dates since I don't want to plot them since they are at the limits of the range\n",
    "        # bounds = [mdates.date2num(i + timedelta(days=1)) for i in list_aggr_date_range[1:-1:2]]\n",
    "        # mdates.date2num(datetime.strptime(i, \"%Y-%m-%d\"))\n",
    "        # out_bounds_min, out_bounds_max = mdates.date2num(list_aggr_date_range[0]), mdates.date2num(list_aggr_date_range[-1])            \n",
    "\n",
    "        \n",
    "        cmap = mcm.get_cmap(colormap)  # define the colormap\n",
    "        # extract all colors from the .jet map\n",
    "        cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "        # create the new map\n",
    "        custom_cmap = mcolors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "        \n",
    "        ## Manually modified the upper limit since only 1 country has 120 days delay, all others are below 105\n",
    "        # bounds = np.arange(0, df_first_day_max_stringency[col_to_plot].max(), bin_width_days)\n",
    "        bounds = np.arange(0, 106, bin_width_days)\n",
    "        \n",
    "        # define the bins and normalize\n",
    "        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "        \n",
    "        # Instead of the interval in days corresponding to the week, get only the corresponding Monday to then substitute the x ticks\n",
    "        # monday_of_week = [(c.left + timedelta(days=1)) for c in res.index.categories]\n",
    "        # monday_of_week_str = [date.strftime('%d.%m') for date in monday_of_week]\n",
    "        \n",
    "        ## Plotting\n",
    "        \n",
    "        # fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,5))\n",
    "\n",
    "        ## Group dates of max stringency by week number of the year\n",
    "        # count_first_day_max_stringency_per_week = df_first_day_max_stringency.groupby(df_first_day_max_stringency.dt.week).count()\n",
    "        # count_first_day_max_stringency_per_week.plot(kind='bar', ax=ax)\n",
    "        # display(df_first_day_max_stringency.apply(lambda x: x - timedelta(days=x.weekday())))\n",
    "\n",
    "        ## Define colors for the bars from the custom colormap. Bars have the same color 2 by 2 since once bar is one week while the colormap has one color for 2 weeks.\n",
    "        # custom_colors = [custom_cmap(norm(mdates.date2num(c.left + timedelta(days=1)))) for c in res_aggr.index.categories]            \n",
    "        \n",
    "        # indx_nnz = (gdf_merged['first_day_max_' + col_df_cases] - gdf_merged['first_day_max_stringency']) > 0\n",
    "        \n",
    "        indx_null = gdf_merged[col_ref].isnull()\n",
    "        # indx_ls_min_bounds = gdf_merged['first_day_max_stringency_numerical'] < mdates.date2num(datetime.strptime(bottom_date, \"%Y-%m-%d\"))\n",
    "        # indx_gt_max_bounds = gdf_merged['first_day_max_stringency_numerical'] > mdates.date2num(datetime.strptime(top_date, \"%Y-%m-%d\"))\n",
    "        indx_ls_min_bounds = gdf_merged[col_ref] < bottom_date\n",
    "        indx_gt_max_bounds = gdf_merged[col_ref] > top_date\n",
    "\n",
    "\n",
    "        ## Plot countries colored in grey if out of the dates in bounds\n",
    "        # gdf_merged.loc[indx_ls_min_bounds].plot(ax=ax, color='0.9', edgecolor='grey', linewidth=0.5)\n",
    "        \n",
    "        ## Do not plot, instead replace null values\n",
    "        # if indx_ls_min_bounds.sum() > 0:\n",
    "        #     gdf_merged.loc[indx_ls_min_bounds].plot(ax=ax, color='red', edgecolor='grey', linewidth=0.5)\n",
    "        # if indx_gt_max_bounds.sum() > 0:\n",
    "        #    gdf_merged.loc[indx_gt_max_bounds].plot(ax=ax, color='black', edgecolor='grey', linewidth=0.5)\n",
    "        \n",
    "        gdf_merged.loc[indx_ls_min_bounds][col_to_plot] = np.nan\n",
    "        gdf_merged.loc[indx_gt_max_bounds][col_to_plot] = np.nan\n",
    "\n",
    "        gdf_merged.loc[indx_ls_min_bounds][col_to_plot.strip(\"_numerical\")] = pd.NaT\n",
    "        gdf_merged.loc[indx_gt_max_bounds][col_to_plot.strip(\"_numerical\")] = pd.NaT\n",
    "        \n",
    "        # countries_not_to_plot_cases = gdf_merged.loc[gdf_merged['first_day_max_stringency_numerical'].isnull()]['iso_a3'].values().tolist() + gdf_merged.loc[gdf_merged['first_day_max_stringency_numerical'] < min(bounds)]['iso_a3'].values().tolist() + gdf_merged.loc[gdf_merged['first_day_max_stringency_numerical'] > max(bounds)]['iso_a3'].values().tolist()\n",
    "        \n",
    "        ## Plot colors by delay between max cases per 100k population and first day of max stringency\n",
    "        # gdf_merged.loc[~(indx_ls_min_bounds | indx_gt_max_bounds)].plot(column=col_to_plot,\n",
    "        #                                                                 ax=ax,\n",
    "        #                                                                 cmap=custom_cmap,\n",
    "        #                                                                 norm=norm,\n",
    "        #                                                                 edgecolor='grey', linewidth=0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Substituting the line below for coloring with drawing hatches\n",
    "        # gdf_merged.loc[indx_null].plot(ax=ax, color='0.7', edgecolor='grey', linewidth=0.5)\n",
    "        # matplotlib.rcParams['hatch.linewidth'] = 0.25\n",
    "        # gdf_merged.loc[indx_null].apply(lambda x : ax.add_patch(PolygonPatch(x['geometry'],\n",
    "        #                                                                      fill=False,\n",
    "        #                                                                      edgecolor='grey',\n",
    "        #                                                                      hatch='|'*10+'-'*10,\n",
    "        #                                                                      linewidth=0.5)), axis=1)\n",
    "        \n",
    "        # divider = make_axes_locatable(ax)\n",
    "        # cax = divider.append_axes(\"right\", size=\"2%\", pad=0)\n",
    "\n",
    "        # colbar = matplotlib.colorbar.ColorbarBase(cax, \n",
    "        #                                           cmap=custom_cmap, \n",
    "        #                                           norm=norm, \n",
    "        #                                           spacing='uniform', \n",
    "        #                                           ticks=bounds)\n",
    "\n",
    "        # custom_cmap.set_over('cyan')\n",
    "        custom_cmap.set_under('mediumseagreen')\n",
    "        \n",
    "        # matplotlib.rcParams['hatch.linewidth'] = 0.25\n",
    "        # gdf_merged.loc[gdf_merged[col_to_plot]<0].apply(lambda x : ax.add_patch(PolygonPatch(x['geometry'],\n",
    "        #                                                                                      fill=False,\n",
    "        #                                                                                      edgecolor='grey',\n",
    "        #                                                                                      hatch='|'*6+'-'*6,\n",
    "        #                                                                                      linewidth=0.5)), axis=1)\n",
    "\n",
    "        \n",
    "        gdf_merged['color_delay_sim_max_stringency'] = gdf_merged[col_to_plot].apply(lambda x: mcolors.to_hex(custom_cmap(norm(x))))\n",
    "        \n",
    "        # colbar.ax.tick_params(labelsize=ticksize)\n",
    "        # colbar.ax.set_title('Num. of days', fontsize=ticksize, weight='bold')\n",
    "\n",
    "  \n",
    "        # ax.set_title('Delay between day of maximum cases and first day of high stringency index', weight='bold', fontsize=fs)\n",
    "    \n",
    "        ## Hide ticks and labels\n",
    "        # ax.axis('off')\n",
    "\n",
    "        # fig_path = os.path.join(figures_dir, 'mobility_cases_stringency', fig_name)\n",
    "        # fig.tight_layout()\n",
    "        # fig.savefig(fig_path, dpi=300, bbox_inches=0, pad=0)\n",
    "    \n",
    "        # save_path = os.path.join(data_dir, \"geodata_delay_days_max_cases_high_stringency.geojson\")\n",
    "        # gdf_merged.to_file(save_path, driver='GeoJSON')        \n",
    "    \n",
    "    return gdf_merged, bottom_date, top_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf_delay, bottom_date, top_date = save_geodataframe_delay_max_cases_per_100k_and_first_day_max_stringency(gdf_world, \n",
    "                                                                        df_ox, \n",
    "                                                                        df_cases, 'new_cases_per_100000', \n",
    "                                                                        '2020-03-01', '2020-06-30', \n",
    "                                                                        7, \n",
    "                                                                        'plasma_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select columns to rename in order to later merge them in a single geodataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_df_cases = 'new_cases_per_100000'\n",
    "\n",
    "cols_rename_gdf_delay = ['first_day_max_stringency', 'first_day_max_stringency_numerical', \n",
    "                         'first_day_max_' + col_df_cases, \n",
    "                         'max_stringency', \n",
    "                         'first_day_sim_max_stringency', \n",
    "                         # 'delay_day_max_' + col_df_cases + '_first_day_max_stringency',\n",
    "                         'delay_day_max_' + col_df_cases + '_first_day_max_stringency_numerical',\n",
    "                         # 'delay_day_max_' + col_df_cases + '_first_day_sim_max_stringency',\n",
    "                         'delay_day_max_' + col_df_cases + '_first_day_sim_max_stringency_numerical',\n",
    "                         'color_delay_sim_max_stringency']\n",
    "\n",
    "dict_cols_rename_gdf_delay = {k:k + \"_{0}_{1}\".format(bottom_date, top_date) for k in cols_rename_gdf_delay}\n",
    "\n",
    "gdf_delay = gdf_delay.rename(columns = dict_cols_rename_gdf_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge geodataframes created in the 2 functions `save_geodataframe_` above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all_info = pd.merge(gdf_first_day_max_stringency_cases_per_100k, \n",
    "                        gdf_delay[list(dict_cols_rename_gdf_delay.values()) + ['iso_a3']], \n",
    "                        left_on = 'iso_a3', \n",
    "                        right_on = 'iso_a3', how='left')\n",
    "\n",
    "save_path = os.path.join(data_dir, \"geodata_COVID-19_stringency_delay.geojson\")\n",
    "gdf_all_info.to_file(save_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap of cases_per_100000 and stringency for high, middle and low income countries (10 countries per each group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gdp_per_capita_categories(df_input, df_gdp_per_capita, population_threshold):\n",
    "\n",
    "    # date_min = datetime(2020, 1, 1)\n",
    "    # date_max = datetime(datetime.now().year,datetime.now().month,datetime.now().day) - timedelta(days=1)\n",
    "    # date_max = datetime(2020, 10, 16) - timedelta(days=1)\n",
    "    \n",
    "    # dd_date_min, mm_date_min = date_min.split('-')[2], date_min.split('-')[1]\n",
    "    # dd_mm_date_min = \"-\".join([dd_date_min, mm_date_min])\n",
    "\n",
    "    # dd_date_max, mm_date_max = date_max.split('-')[2], date_max.split('-')[1]\n",
    "    # dd_mm_date_max = \"-\".join([dd_date_max, mm_date_max])\n",
    "\n",
    "    \n",
    "    # Not all the countries (e.g. territories) from gdf_input are present in df_stringency or df_cases, so keep only those that are there!\n",
    "    # indx_countries_in_stringeny_cases = gdf_input['iso_a3'].isin(df_stringency['CountryCode']) & gdf_input['iso_a3'].isin(df_cases['iso_code'])\n",
    "    # gdf_plot = gdf_input.loc[indx_countries_in_stringeny_cases]\n",
    "\n",
    "    indx_countries_stringency_cases_notnull = df_input['iso_code'].notnull() & df_input['CountryCode'].notnull()\n",
    "    df_countries_stringency_cases_notnull = df_input.loc[indx_countries_stringency_cases_notnull]\n",
    "\n",
    "    df_unique_countries = df_countries_stringency_cases_notnull.drop_duplicates(subset=['iso_code', 'population'])\n",
    "    \n",
    "    ## Filter on country population\n",
    "    # gdf_plot = gdf_plot.loc[gdf_plot['pop_est'] > 1*(10**6)]\n",
    "    df_subset = df_unique_countries.loc[df_unique_countries['population'] > population_threshold]\n",
    "\n",
    "    # gdf_plot['gdp_per_capita'] = gdf_plot['gdp_md_est'] / gdf_plot['pop_est']\n",
    "    ## Add GDP per capita from World Bank in 2019\n",
    "    # gdf_plot['gdp_per_capita'] = gdf_plot['iso_a3'].map(pd.Series(df_gdp_per_capita['2019'].values, index=df_gdp_per_capita['Country Code']))\n",
    "    df_subset['gdp_per_capita'] = df_subset['iso_code'].map(pd.Series(df_gdp_per_capita['2019'].values, index=df_gdp_per_capita['Country Code']))\n",
    "    \n",
    "    ## Compute division in high, low, and middle income countries\n",
    "    df_subset['category_gdp_per_capita'] = pd.qcut(df_subset['gdp_per_capita'], 3, labels=['poor', 'middle', 'rich']) # retbins=True,\n",
    "        \n",
    "    ## Store in df_input 'category_gdp_per_capita' per each country\n",
    "    df_input['category_gdp_per_capita'] = df_input['iso_code'].map(pd.Series(df_subset['category_gdp_per_capita'].values, index=df_subset['iso_code']))\n",
    "        \n",
    "    ## Add a column which is True if the country has been selected to be plotted in the heatmap, False otherwise\n",
    "    df_input['country_to_plot'] = False\n",
    "    \n",
    "    for i, gdp_cat in enumerate(['rich', 'middle', 'poor']):\n",
    "        \n",
    "        df_subset_gdp_cat = df_subset.loc[df_subset['category_gdp_per_capita'] == gdp_cat]\n",
    "        countries_gdp_cat = df_subset_gdp_cat['iso_code']\n",
    "        df_selected_countries = df_subset_gdp_cat.nlargest(10, 'gdp_per_capita', keep='all')\n",
    "        top_gdp_per_capita_countries_code = df_selected_countries['iso_code']\n",
    "        \n",
    "        if gdp_cat == 'rich':\n",
    "            # Manually adding a bunch of countries for high-income\n",
    "            list_index = []\n",
    "            list_sel_ccs = ['ITA', 'DEU', 'ESP', 'GBR', 'FRA']\n",
    "            for sel_cc in list_sel_ccs:\n",
    "                list_index.append(countries_gdp_cat.loc[countries_gdp_cat == sel_cc].index[0])\n",
    "            top_gdp_per_capita_countries_code = top_gdp_per_capita_countries_code.drop(top_gdp_per_capita_countries_code.loc[top_gdp_per_capita_countries_code == \"QAT\"].index, axis=0)\n",
    "            top_gdp_per_capita_countries_code = top_gdp_per_capita_countries_code.append(pd.Series(list_sel_ccs, index=list_index), verify_integrity=True)\n",
    "                \n",
    "        df_input.loc[df_input['iso_code'].isin(top_gdp_per_capita_countries_code), 'country_to_plot'] = True\n",
    "        \n",
    "        # print(i, df_selected_countries['countriesAndTerritories'].values.tolist())  \n",
    "        \n",
    "    return df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_merged_cases_stringency = add_gdp_per_capita_categories(df_merged_cases_stringency, df_gdp_per_capita, 1*(10**6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save merged datasets for COVID-19 and stringency index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Renaming dictionary and list of columns to save for ECDC dataset\n",
    "\n",
    "dict_column_rename = {'dateRep':'date', \n",
    "                      'countryterritoryCode':'iso_a3', 'geoId':'iso_a2', 'countriesAndTerritories':'country', \n",
    "                      'continentExp':'continent',\n",
    "                      'StringencyIndexForDisplay':'stringency_index'}\n",
    "\n",
    "columns_to_save_df_merged_cases_stringency = ['date', 'cases', 'deaths', \n",
    "                                              'cases_per_100000', 'deaths_per_100000', \n",
    "                                              'cases_7_day_average', 'deaths_7_day_average', \n",
    "                                              'cases_per_100000_7_day_average', 'deaths_per_100000_7_day_average',\n",
    "                                              'Cumulative_number_for_14_days_of_COVID-19_cases_per_100000',\n",
    "                                              'popData2019', 'continent', 'stringency_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_column_rename = {'StringencyIndexForDisplay':'stringency_index'}\n",
    "\n",
    "df_merged_cases_stringency = df_merged_cases_stringency.rename(columns = dict_column_rename)\n",
    "\n",
    "df_merged_cases_stringency.to_csv(os.path.join(data_dir, 'COVID-19_stats_stringency_index.csv'), index=False)\n",
    "                                  # columns=columns_to_save_df_merged_cases_stringency,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-magicbox-reports",
   "language": "python",
   "name": "venv-magicbox-reports"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
